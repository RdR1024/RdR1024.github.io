<?xml version="1.0" encoding="utf-8"?>
<rss xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Richard de Rozario</title><link>http://richardderozario.org/</link><description>Living the analytics life in Melbourne</description><item><title>About me</title><link>http://richardderozario.org/posts/AboutMe/AboutMe.html</link><guid>http://richardderozario.org/posts/AboutMe/AboutMe.html</guid><pubDate>Sat, 01 Jan 2011 00:00:00 +1100</pubDate><description></description><content:encoded><![CDATA[
<div id="header">
<h1 class="title">About me</h1>
</div>
<p>Currently, I lead an operational risk team for a major bank, focusing on the statistical modelling and management of extreme risk scenarios, such as cybercrimes, rogue traders and natural disasters.</p>
<p>For the last 25+ years I've led analytical functions, especially related to risk, business operations and information systems. Theorywise, I studied information systems at <a href="http://www.weber.edu/">Weber State University</a>, hold a postgraduate diploma in market modelling from <a href="http://www.swinburne.edu.au/">Swinburne University</a> and completed my PhD in the applied logic of business intelligence at the <a href="http://www.unimelb.edu.au/">University of Melbourne</a>.</p>
<div class="references">

</div>
<!-- GOOGLE ANALYTICS -->
<script><![CDATA[
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-60473518-1', 'auto');
  ga('send', 'pageview');

]]]]><![CDATA[></script>]]></content:encoded></item><item><title>The Completeness of Categories</title><link>http://richardderozario.org/posts/CompletenessOfCategories/CompletenessOfCategories.html</link><guid>http://richardderozario.org/posts/CompletenessOfCategories/CompletenessOfCategories.html</guid><pubDate>Wed, 14 Jan 2015 00:00:00 +1100</pubDate><description>Calculating how many categories are needed to reach a certain confidence level, based on some simplified assumptions.</description><content:encoded><![CDATA[
<div id="header">
<h1 class="title">The Completeness of Categories</h1>
<h3 class="date">2015-01-14</h3>
</div>
<p>Lately, I've been wrestling with the question of "how many categories are enough?" The start of many a risk analysis is to categorise the risks. A categorisation serves as a checklist for the completeness of the analysis, and as a way of organising the many possible risks. Typically, the categories capture some essential aspect of a causal mechanism or effect which typifies the risk. For example, we might use "Internal Fraud" and "External Fraud" as a categories, thus distinguishing different causes of a particular financial impact. Basically, risk categories are shorthand descriptions of groups of similar risks. But how do we create a good categorisation, or at least avoid a "Celestial Empire of Benevolent Knowledge"<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a>?</p>
<p>In <a href="http://timvangelder.com/2010/06/04/what-is-mece-and-is-it-mece/#comment-959">Tim's blog</a>, I commented that it is not difficult to create categories that follow at least the criteria that have been known since the 13th Century. That is, to create a set of categories that are complete and mutually exclusive, start with any set of predicates (yes/no attributes) and allow the objects of interest to be tagged with as many of the predicates as are applicable. The categories are then formed by the possible combinations of predicates. The third criteria is fulfilled to the extent that the predicates are applicable to the objects of interest. As an example, if we wish to categorise coloured balls, then start with predicates Red, Green and Blue and then categorise balls according to the presence of each primary colour.</p>
<p>However, there is still a question of "how many categories do we need?" Here, the principle of focusing on the most material risks comes into play.</p>
<p>First, let's flesh out a bit more the link between categories and risk magnitudes. Imagine that the collection of all the possible risk magnitudes forms a distribution. That is, every risk has a magnitude and a relative frequency of occurrence. Furthermore, assume that ultimately we are interested in identifying the most material risk. Often, there is no theoretical maximum, so "most material" means some confidence level like "the value such that 99.9% of all values are less or equal." Also, assume that categories can be as fine-grained as we like &mdash; in the extreme, that each risk has its own category. Lastly, assume that we can only come up with an <em>arbitrary</em><a href="#fn2" class="footnoteRef" id="fnref2"><sup>2</sup></a> set of categories, but that we know enough to rank them in order of magnitude.</p>
<p>In essence, what those categories represent is an random, but ordered set of points on the magnitude distribution. And the question of interest is, what is the expected number of points needed to reach the confidence level?</p>
<p>With a bit of R-code, we can calculate that. We'll calculate a list of a random points along the distribution (i.e. random "quantiles"). The list will be of random length. Then we simulate that a large number of times and calculate the average (i.e. "expected") length. For good measure, we can also calculate an upper limit (confidene level) on the length of the list. These lengths tell us the number of catgegories that we need.</p>
<pre class="sourceCode r"><code class="sourceCode r">    <span class="co"># a list of random, ordered quantiles</span>
    froq &lt;-<span class="st"> </span>function(Q){
        Len &lt;-<span class="st"> </span><span class="dv">0</span>
        Rnd &lt;-<span class="st"> </span><span class="dv">0</span>
        while(Rnd &lt;=<span class="st"> </span>Q) {
            Rnd &lt;-<span class="st"> </span><span class="kw">runif</span>(<span class="dv">1</span>,Rnd)
            Len &lt;-<span class="st"> </span>Len<span class="dv">+1</span>
        }
        <span class="kw">return</span>(Len)
    }
    
    <span class="co"># Simulate large number of times. Get average and upper confidence level.</span>
    X &lt;-<span class="st"> </span><span class="kw">replicate</span>(<span class="fl">1e5</span>,<span class="kw">froq</span>(<span class="fl">0.999</span>))
    Expected &lt;-<span class="st"> </span><span class="kw">mean</span>(X)
    Upper &lt;-<span class="st"> </span><span class="kw">quantile</span>(X,<span class="fl">0.999</span>)</code></pre>
<p>So, by this analysis, we need between 8 and 17 categories to have confidence that our categories cover the most material magnitude.</p>
<div class="references">

</div>
<div class="footnotes">
<hr /><ol><li id="fn1"><p>Borges critiques categorisation as a form of knowledge with an fictional example of a particularly silly set of categories: <em>&#8230;a certain Chinese encyclopedia entitled "Celestial Empire of benevolent Knowledge". In its remote pages it is written that the animals are divided into: (a) those that belong to the emperor, (b) embalmed ones, (c) those that are trained, (d) sucking pigs, (e) mermaids,(f) fabulous ones, (g) stray dogs, (h) those that are included in this classification, (i) those that tremble as if they were mad, (j) innumerable ones, (k) those drawn with a very fine camel's hair brush, (l) others, (m) those that have just broken a flower vase, (n) those that resemble flies from a distance.</em> <strong>Borges, J.L.</strong> (1952, p.103), "The analytical language of John Wilkins", <em>Other Inquisitions 1937-1952</em>, Souvenir Press, London, 1973<a href="#fnref1"><U+21A9></a></p></li>
<li id="fn2"><p>As always, there is a caveat here: if the process that generates the categories does <em>not</em> generate an arbitrary (that is, randomly distributed) set of categories, then the analysis doesn't hold. So, we may have to take steps to counter the "bias"" in the process. Note that for the purposes of covering the "most material" magnitude, the counter need only be in one direction. So, for example we may deliberately chose a high threshold to begin the analysis.<a href="#fnref2"><U+21A9></a></p></li>
</ol></div>
<!-- GOOGLE ANALYTICS -->
<script><![CDATA[
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-60473518-1', 'auto');
  ga('send', 'pageview');

]]]]><![CDATA[></script>]]></content:encoded></item><item><title>K-ary tree level probability</title><link>http://richardderozario.org/posts/karyl/karyl.html</link><guid>http://richardderozario.org/posts/karyl/karyl.html</guid><pubDate>Sat, 07 Feb 2015 00:00:00 +1100</pubDate><description>probability distribution of k-ary tree levels, with application for hierarchical processes.</description><content:encoded><![CDATA[
<div id="header">
<h1 class="title">K-ary tree level probability</h1>
<h3 class="date">2015-02-07</h3>
</div>
<p>On the weekend, I ran into a problem that needed a probability distribution that I hadn't seen before. I googled around, but couldn't find any implementation of what I needed. It's probably out there, somewhere on page umpteen of the search results, but I figured it was a good opportunity to implement a custom distribution in R.</p>
<p>The context is a hierarchy, like an area of business operations, with processes that are hierarchically organised. That is, the bottom level is the processing of transactions, but then there are higher level processes that affect the transaction process. For example, setting fees across groups of transactions. Then there are even higher level processes, say, setting the criteria for market segments, within which we might have different groups of fees.</p>
<p>Now imagine that events can happen across the area at random. That means sometimes the event will touch a transaction at the bottom level, but sometimes at a higher level. If the event occurs at a higher level, then that will affect multiple transactions at the bottom level. So, what I want to know is, what is the probability that an event will happen at a particular level, and how many bottom level transactions will be affected by that event?</p>
A simple model with a fixed number of lower level transactions enables us to calculate the probability that a given number of transactions are affected.<br /><figure><center>
<img title="plot of chunk simple_tree" alt="plot of chunk simple_tree" width="288" src="http://richardderozario.org/posts/karyl/figure/simple_tree-1.png" /><figcaption><em>Figure 1. Simple process hierarchy as a k-ary tree. Bottom nodes are individual transactions affected.</em>
</figcaption></center>
</figure><p>Using the <em>diagram</em> package <span class="citation">(Soetaert 2009)</span>, I have drawn a <a href="http://en.wikipedia.org/wiki/Hasse_diagram">Hasse</a> diagram in <em>Figure 1</em> of a small example hierarchy of 13 possible events, some of which are higher level, which means they affect multiple lower-level transactions. In this example, it's easy to see that there are 4 possible events where more than a single transaction is affected. This type of hierarchy is called an <a href="http://en.wikipedia.org/wiki/K-ary_tree">k-ary tree</a>. For the purpose of estimating the number of transactions affected by a higher level event, we can turn the tree into a a probability function.</p>
<p>A convenient form of the probability function is the probability that a node is on a particular level, given a fixed group size (the number of nodes belonging to a direct parent node) and the maximum number of levels. To derive the probability mass function, we use the formula that calculates the total number of nodes in the tree (N), where <span class="math">\(L\)</span> is the maximum level, starting from zero at the top node, and <span class="math">\(g\)</span> is the group size:</p>
<p><span class="math">\[
N = f(L,g) =
    \sum^{L}_{i=0} g^i = \frac{g^{L+1} - 1}{g-1}, \quad L,g,i \in \mathbb{N}_0, g&gt;1
\]</span></p>
<p>Each node on a level other than the last level is a tree in itself. So, the number of end-nodes that a node on a given level <span class="math">\(i\)</span> will reach is calculated as <span class="math">\(g^{L-i}\)</span>, and the probability that a node is on level <span class="math">\(i\)</span> is given by the following probability mass function (PMF):</p>
<p><span class="math">\[
Pr(i;L,g) = \frac{1}{N}g^i = \frac{g^i(g-1)}{g^{L+1}-1}, \quad i&lt;=L
\]</span></p>
<p>The corresponding cumulative distribution function (CDF) is:</p>
<p><span class="math">\[
Pr(i&lt;=n;L,g) = \frac{1}{N}\sum_{i=0}^n g^i = \frac{g^{n+1}-1}{g^{L+1}-1}
\]</span></p>
<p>The full set of probability functions for the k-ary levels can be coded in R as follows:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># k-ary tree level probability functions (without error checks)</span>
dkaryl &lt;-<span class="st"> </span>function(i,L,g) (g^i *<span class="st"> </span>(g<span class="dv">-1</span>)) /<span class="st"> </span>(g^(L<span class="dv">+1</span>) -<span class="dv">1</span>)
pkaryl &lt;-<span class="st"> </span>function(n,L,g) (g^(n<span class="dv">+1</span>) -<span class="dv">1</span>) /<span class="st"> </span>(g^(L<span class="dv">+1</span>) -<span class="dv">1</span>)
qkaryl &lt;-<span class="st"> </span>function(q,L,g) <span class="kw">floor</span>((<span class="kw">log</span>(q*(g^(L<span class="dv">+1</span>)-<span class="dv">1</span>)+<span class="dv">1</span>)-<span class="kw">log</span>(g))/<span class="kw">log</span>(g))
rkaryl &lt;-<span class="st"> </span>function(n,L,g) <span class="kw">sample</span>(<span class="dv">0</span>:L,n,<span class="dt">replace=</span><span class="ot">TRUE</span>,<span class="dt">prob=</span><span class="kw">dkaryl</span>(<span class="dv">0</span>:L,L,g))</code></pre>
Note that the probability functions range over the number of levels in a tree, which may not be all that many. The following example draws a histogram for a tree with 10 levels and groups of 5.
<figure><pre class="sourceCode r"><code class="sourceCode r">x &lt;-<span class="st"> </span><span class="kw">rkaryl</span>(<span class="fl">1e3</span>,<span class="dv">10</span>,<span class="dv">5</span>)
<span class="kw">hist</span>(x,<span class="dt">main=</span><span class="st">"k-ary tree level probability mass"</span>,
     <span class="dt">xlab=</span><span class="st">"level"</span>,<span class="dt">col=</span><span class="st">"grey"</span>,<span class="dt">xlim=</span><span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">10</span>))</code></pre>
<img src="http://richardderozario.org/posts/karyl/figure/example-1.png" /><figcaption><em>Figure 2. histogram of k-ary level probability density with max level of 10 and groups of 5.</em>
</figcaption></figure><p><em>Bibliography</em></p>
<div class="references">
<p>Soetaert, Karline. 2009. "R Package Diagram: Visualising Simple Graphs, Flowcharts, and Webs." software library; CRAN.</p>
</div>
<!-- GOOGLE ANALYTICS -->
<script><![CDATA[
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-60473518-1', 'auto');
  ga('send', 'pageview');

]]]]><![CDATA[></script>]]></content:encoded></item><item><title>Melbourne Holidays</title><link>http://richardderozario.org/posts/MelbourneHolidays/MelbourneHolidays.html</link><guid>http://richardderozario.org/posts/MelbourneHolidays/MelbourneHolidays.html</guid><pubDate>Sun, 01 Feb 2015 00:00:00 +1100</pubDate><description>How to calculate the public holidays of a given year for Melbourne, Australia</description><content:encoded><![CDATA[
<div id="header">
<h1 class="title">Melbourne Holidays</h1>
<h3 class="date">2015-02-01</h3>
</div>
<p>Calendar calculations are probably one of the oldest applications of math to everyday problems. In the past, real-world problems may have driven the calculation of calendars, but these days the reverse may also be true. Recently, I was looking at a time-series that seemed to have calendar effects: volumes of activities varied relative to public holidays.</p>
<p>Since I use R for my analysis, I needed a function to calculate the public holidays for Melbourne, which I'll describe here. Firstly, I use a the <em>lubridate</em> package for various date formats.</p>
<pre class="sourceCode r"><code class="sourceCode r">    <span class="co"># Melbourne public holidays</span>
    <span class="kw">require</span>(lubridate, <span class="dt">quietly=</span><span class="ot">TRUE</span>)</code></pre>
<p>Next, we need a function to calculate the Easter date. Some years ago, there was a competition for the shortest formulas for Easter calculations in Excel. I translated <a href="http://www.contextures.com/exceleastercalculation.html">Roger Friederich</a>'s contribution to R:</p>
<pre class="sourceCode r"><code class="sourceCode r">    <span class="co"># Easter calculation</span>
    Easter &lt;-<span class="st"> </span>function(<span class="dt">year=</span><span class="dv">1971</span>){
        d &lt;-<span class="st"> </span>((year %%<span class="st"> </span><span class="dv">19</span>) *<span class="st"> </span><span class="fl">18.37</span> -<span class="dv">6</span>) %%<span class="st"> </span><span class="dv">29</span>
        s &lt;-<span class="st"> </span><span class="kw">as.numeric</span>(<span class="kw">as.Date</span>(<span class="kw">paste</span>(year,<span class="dv">3</span>,d,<span class="dt">sep=</span><span class="st">"-"</span>))) +<span class="st"> </span><span class="dv">5</span>
        s &lt;-<span class="st"> </span>(s %/%<span class="st"> </span><span class="dv">7</span> *<span class="st"> </span><span class="dv">7</span> +<span class="st"> </span><span class="dv">24</span>)
        <span class="kw">return</span>(<span class="kw">format</span>(<span class="kw">as.Date</span>(s,<span class="dt">origin=</span><span class="st">"1970-01-01"</span>),<span class="st">"%d-%m-%Y"</span>))
    }</code></pre>
<p>This function tells us that, for example, the date of Easter Sunday in 2015 falls on 2015-04-05.</p>
<p>A number of public holidays are calculated as some n<sup>th</sup> weekday of a month. For example, Labour Day is the second Monday in February. So, we need an n<sup>th</sup> day function:</p>
<pre class="sourceCode r"><code class="sourceCode r">    <span class="co"># calculate date of nth weekday in a month</span>
    <span class="co"># example: 2nd Monday in the month</span>
    <span class="co"># daynum is Sun=1, Mon=2, Tue=3,...</span>
    nthday &lt;-<span class="st"> </span>function(daynum, n, month, year){
        Monthstart &lt;-<span class="st"> </span><span class="kw">dmy</span>(<span class="kw">paste</span>(<span class="dv">1</span>,month,year,<span class="dt">sep=</span><span class="st">"-"</span>))
        firstday &lt;-<span class="st"> </span><span class="kw">wday</span>(Monthstart)
        delta &lt;-<span class="st"> </span>(daynum +<span class="st"> </span><span class="dv">7</span> -<span class="st"> </span>firstday) %%<span class="st"> </span><span class="dv">7</span> +<span class="st"> </span>(n<span class="dv">-1</span>)*<span class="dv">7</span>
        <span class="kw">return</span>(Monthstart +<span class="st"> </span><span class="kw">days</span>(delta))
    }</code></pre>
<p>Now we're ready to put it all together. The final function will produce a simple list of the "effective"" date of the holidays. Effective date is the the actual free workday that occurs (which may be the Monday or Tuesday after the public holiday if it falls on a weekend).</p>
<p>Unfortunately, there is an exception for ANZAC day, which does not result in a free workday if it happens to fall on a weekend. This ruling depends political rulings, so needs to be hard coded.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># get list of holidays for given year</span>
    melbhols &lt;-<span class="st"> </span>function(<span class="dt">year=</span><span class="dv">1971</span>){
        hols &lt;-<span class="st"> </span><span class="kw">dmy</span>(<span class="kw">paste</span>( <span class="dv">1</span>,<span class="dv">1</span>, year, <span class="dt">sep=</span><span class="st">"-"</span>))
        hols &lt;-<span class="st"> </span><span class="kw">c</span>(hols, <span class="kw">dmy</span>(<span class="kw">paste</span>( <span class="dv">26</span>,<span class="dv">1</span>, year, <span class="dt">sep=</span><span class="st">"-"</span>)))
        hols &lt;-<span class="st"> </span><span class="kw">c</span>(hols, <span class="kw">nthday</span>(<span class="dv">2</span>,<span class="dv">2</span>,<span class="dv">3</span>,year))
    
        EasterSunday &lt;-<span class="st"> </span><span class="kw">dmy</span>(<span class="kw">Easter</span>(year))
        hols &lt;-<span class="st"> </span><span class="kw">c</span>(hols, EasterSunday +<span class="st"> </span><span class="kw">ddays</span>(-<span class="dv">2</span>))
        hols &lt;-<span class="st"> </span><span class="kw">c</span>(hols, EasterSunday +<span class="st"> </span><span class="kw">ddays</span>(<span class="dv">1</span>))
    
        hols &lt;-<span class="st"> </span><span class="kw">c</span>(hols, <span class="kw">dmy</span>(<span class="kw">paste</span>(<span class="dv">25</span>,<span class="dv">4</span>,year,<span class="dt">sep=</span><span class="st">"-"</span>)))
        hols &lt;-<span class="st"> </span><span class="kw">c</span>(hols, <span class="kw">nthday</span>(<span class="dv">2</span>,<span class="dv">2</span>,<span class="dv">6</span>,year))
        hols &lt;-<span class="st"> </span><span class="kw">c</span>(hols, <span class="kw">nthday</span>(<span class="dv">3</span>,<span class="dv">1</span>,<span class="dv">11</span>,year))
        hols &lt;-<span class="st"> </span><span class="kw">c</span>(hols, <span class="kw">dmy</span>(<span class="kw">paste</span>(<span class="dv">25</span>,<span class="dv">12</span>,year,<span class="dt">sep=</span><span class="st">"-"</span>)))
        hols &lt;-<span class="st"> </span><span class="kw">c</span>(hols, <span class="kw">dmy</span>(<span class="kw">paste</span>(<span class="dv">26</span>,<span class="dv">12</span>,year,<span class="dt">sep=</span><span class="st">"-"</span>)))
    
        <span class="kw">names</span>(hols) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">"NewYears"</span>,<span class="st">"AusDay"</span>,<span class="st">"LabourDay"</span>,<span class="st">"GoodFriday"</span>,
                     <span class="st">"EasterMonday"</span>,<span class="st">"Anzac"</span>,<span class="st">"QueensBD"</span>,<span class="st">"CupDay"</span>,
                     <span class="st">"Xmas"</span>,<span class="st">"BoxingDay"</span>)
    
        <span class="co"># adjust for holidays on weekends</span>
        nh &lt;-<span class="st"> </span><span class="kw">length</span>(hols)
        for(h in <span class="dv">1</span>:nh){
            if(h !=<span class="st"> </span><span class="dv">6</span>){  <span class="co"># days in lieu, except for ANZAC</span>
                wd &lt;-<span class="st"> </span><span class="kw">wday</span>(hols[h])
                if(wd==<span class="dv">7</span>) hols[h] &lt;-<span class="st"> </span>hols[h] +<span class="st"> </span><span class="kw">ddays</span>(<span class="dv">2</span>)
                if(wd==<span class="dv">1</span>) hols[h] &lt;-<span class="st"> </span>hols[h] +<span class="st"> </span><span class="kw">ddays</span>(<span class="dv">1</span>)
            }
        }
        if(hols[nh]==hols[nh<span class="dv">-1</span>]) hols[nh] &lt;-<span class="st"> </span>hols[nh] +<span class="st"> </span><span class="kw">ddays</span>(<span class="dv">1</span>)
        
        <span class="kw">return</span>(<span class="kw">format</span>(<span class="kw">as.Date</span>(hols),<span class="st">"%d-%m-%Y"</span>))
    }</code></pre>
<p>So, for 2015 the public holidays are as follows:</p>
<pre class="sourceCode r"><code class="sourceCode r">    <span class="kw">melbhols</span>(<span class="dv">2015</span>)</code></pre>
<pre><code>##     NewYears       AusDay    LabourDay   GoodFriday EasterMonday 
## "01-01-2015" "26-01-2015" "09-03-2015" "03-04-2015" "06-04-2015" 
##        Anzac     QueensBD       CupDay         Xmas    BoxingDay 
## "25-04-2015" "08-06-2015" "03-11-2015" "25-12-2015" "28-12-2015"</code></pre>
<div class="references">

</div>
<!-- GOOGLE ANALYTICS -->
<script><![CDATA[
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-60473518-1', 'auto');
  ga('send', 'pageview');

]]]]><![CDATA[></script>]]></content:encoded></item><item><title>A mini blog generator in R with knitr and pandoc</title><link>http://richardderozario.org/posts/miniblog/miniblog.html</link><guid>http://richardderozario.org/posts/miniblog/miniblog.html</guid><pubDate>Wed, 25 Feb 2015 00:00:00 +1100</pubDate><description>How to implement a mini blog site generator using R and pandoc. Only html and file folder structure is used to create and maintain a static blog for hosting on sites like GitHub. Aside from speed and simplicity, the advantage is that you can publish directly from R-markdown text.</description><content:encoded><![CDATA[
<div id="header">
<h1 class="title">A mini blog generator in R with knitr and pandoc</h1>
<h3 class="date">2015-02-25</h3>
</div>
<p>When I started blogging about analytics (and R), I went looking for a blogging platform that would allow me to easily post directly from R-markdown. However, the typical choices like Wordpress didn't work well in that regard. I still had to copy, paste and edit the posts, because neither the markdown nor html features from extended markdown were well supported.</p>
<p>It seemed "static blog generators" might be the solution. These (re-)generate the blog as a set of ordinary html pages, which can then be hosted on places like <a href="https://github.com/">GitHub</a>. This means that you can just generate a html page from an R-markdown page, with something like <a href="http://johnmacfarlane.net/pandoc/">pandoc</a>, save it in a folder, and sync the blog page with the host site. Writing a blog becomes almost seamless with writing some literate R.</p>
<p>In this rather long article, I'll document a mini blog generator, written completely in R. The individual blog pages can be standalone html documents, like those generated by <a href="http://yihui.name/knitr/">knitr</a> and pandoc from R-markdown text. Each blog post consists of a folder that contains the html page, with a sub-folder for any graphics &mdash; exactly like knitr generates. The folder may also contain other files, which are ignored by the blog, but enables the folder to be a complete project folder, such as generated by <a href="http://www.rstudio.com/">RStudio</a>. The only restriction is the naming convention: the folder has the same name (minus extension) as the main html page that contains the article. So, an example structure of folders and files might be as follows:</p>
<figure><img src="http://richardderozario.org/posts/miniblog/figure/FolderStructure.png" /><p>
<figcaption><em>Figure 1: example folder structure for the blog</em>
</figcaption></p>
</figure><p>In the example, the top folder<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a> is called <code>username.github.io</code>. That is what you would choose for the top folder name, if you hosted your site using GitHub. That is, when you create an account on GitHub, you would also create a repository named <em>username.github.io</em><a href="#fn2" class="footnoteRef" id="fnref2"><sup>2</sup></a> to host your blog, and that would automatically create a local folder by that name. If you do not host on GitHub, you may call the (local) folder anything you like.</p>
<p>The top folder also contains one essential file: <code>index.html</code>. This is just a straightforward html file, without any special "micro-templating" or "macros". The miniblog generator simply updates this file whenever changes are made in the folders containing posts. Moreover, the generator only touches specific sections of the html, so you're free to get creative with the other parts of the html. That way, you can style the layout of the blog site as you please.</p>
<p>One part that the blog generator changes in the html is the <code>iframe</code> that contains the front page article. This section looks like the following:</p>
<pre><code>
    &lt;!-- front page article --&gt;
    &lt;iframe name="frontpost" class="frontpost" scrolling="auto" 
    frameborder="NO" src="posts/FirstArticle/FirstArticle.html"&gt;&lt;/iframe&gt;
</code></pre>
<p>The generator replaces the <code>src</code> content to the html file of the last article that was updated. Similarly, the generator replaces the links <code>&lt;a...</code> links in the following section:</p>
<pre><code>
    &lt;!-- links to articles --&gt;
    &lt;div class="articles"&gt;
    
    &lt;a href="posts/FirstArticle/FirstArticle.html"
    target="frontpost"&gt;FirstArticle&lt;/a&gt;&lt;br&gt;
    
    &lt;a href="posts/SecondArticle/SecondArticle.html"
    target="frontpost"&gt;SecondArticle&lt;/a&gt;&lt;br&gt;
    ...
    ...
    &lt;/div&gt;
</code></pre>
<p>That's it as far as the blog structure is concerned. An index.html file in the top folder and a sub-folder called "posts" that in turn contains a folder for each article. Articles are just html files that can be produced from R-markdown text using knitr and pandoc.</p>
<p>I prefer using pandoc for producing the final html, because it has nice code formatting, bibliography and can using a YAML-like heading section for meta-data. I also included <a href="https://disqus.com/">Disqus</a> scripts at the end of the pandoc html template to enable comments on the blog pages. A blog post template for pandoc can be found here: <a class="uri" src="http://richardderozario.org/posts/miniblog/blogpost_template.html">blogpost_template.html</a></p>
<p>Now, onto the R-code that generates the blog site. Firstly, we require the XML libraries to handle the html files.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">require</span>(<span class="st">"XML"</span>,<span class="dt">quietly=</span><span class="ot">TRUE</span>)
<span class="kw">require</span>(<span class="st">"selectr"</span>,<span class="dt">quietly=</span><span class="ot">TRUE</span>)</code></pre>
<p>Next is the function that "re-indexes" the front page. By that I mean, we want to scan all the articles in the <code>posts</code> folder and create the links for them in the <code>index.html</code> page. We also want to update the link for the front page article. The list of article links will be in order of last update.</p>
<p>The first parameter of the reindex function is the path to the top folder &mdash; that is, the folder where <code>index.html</code> lives. In our example above, that might be something like <code>c:/GitHub/username.github.io/</code>. The next parameter is the base URL that you would put in a browser to visit the blog &mdash; for example, <code>http://username.github.io/</code>. The last parameter allows you to pass a metadata list &mdash; more about that later.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># recreate index.html with updated article list</span>
<span class="co"># assumes the path points to a folder structure</span>
<span class="co"># that adheres to the miniblog naming convention</span>
reindex &lt;-<span class="st"> </span>function(<span class="dt">path=</span><span class="st">"./"</span>,<span class="dt">base=</span><span class="st">""</span>,<span class="dt">meta=</span><span class="ot">NA</span>){
    docpath &lt;-<span class="st"> </span><span class="kw">paste</span>(path,<span class="st">"index.html"</span>,<span class="dt">sep=</span><span class="st">""</span>)

    if(<span class="kw">file.exists</span>(docpath)){
        doc &lt;-<span class="st"> </span><span class="kw">htmlParse</span>(docpath)

        <span class="co"># get sorted article metadata</span>
    if(<span class="kw">typeof</span>(meta)!=<span class="st">"list"</span>) 
        meta &lt;-<span class="st"> </span><span class="kw">as.data.frame</span>(
            <span class="kw">metadata</span>(<span class="kw">paste</span>(path,<span class="st">"posts/"</span>,<span class="dt">sep=</span><span class="st">""</span>),
                     <span class="kw">paste</span>(base,<span class="st">"posts/"</span>,<span class="dt">sep=</span><span class="st">""</span>)
            )
        )
        p &lt;-<span class="st"> </span><span class="kw">order</span>(meta$mdate,<span class="dt">decreasing=</span><span class="ot">TRUE</span>)

        <span class="co"># replace links to articles</span>
        articles &lt;-<span class="st"> </span><span class="kw">querySelector</span>(doc,<span class="st">"div .articles"</span>)
        <span class="kw">removeNodes</span>(<span class="kw">xmlChildren</span>(articles))
        for(i in p){
            a &lt;-<span class="st"> </span><span class="kw">newXMLNode</span>(<span class="st">"a"</span>, <span class="kw">newXMLTextNode</span>(meta$post[i]), <span class="dt">attrs=</span><span class="kw">c</span>(
                    <span class="dt">href=</span> <span class="kw">paste</span>(<span class="st">"posts/"</span>,meta$relpath[i],<span class="dt">sep=</span><span class="st">""</span>),
                    <span class="dt">target=</span><span class="st">"frontpost"</span>
                    )
                 )
            <span class="kw">addChildren</span>(articles,a)
            <span class="kw">addChildren</span>(articles,<span class="kw">newXMLNode</span>(<span class="st">"br"</span>))
        }
        
        <span class="co"># repoint front page article</span>
        f &lt;-<span class="st"> </span><span class="kw">querySelector</span>(doc,<span class="st">".frontpost"</span>)
        <span class="kw">removeAttributes</span>(f,<span class="dt">.attrs=</span><span class="st">"src"</span>)
        <span class="kw">addAttributes</span>(f,<span class="dt">src=</span><span class="kw">paste</span>(<span class="st">"posts/"</span>,meta$relpath[p[<span class="dv">1</span>]],<span class="dt">sep=</span><span class="st">""</span>))
    } else <span class="kw">warning</span>(<span class="kw">paste</span>(<span class="st">"Index file does not exist: "</span>, docpath,<span class="dt">sep=</span><span class="st">""</span>))
    <span class="kw">return</span>(doc)
}</code></pre>
<p>The function that gathers all the information about the articles is called <code>metadata()</code>. It gets a list of all the article folders in <code>posts</code> and then loops through them. In each folder it sucks in the html document and gathers all the meta data fields in the html header. It also checks the last modification date of the file, so that we can sort later by "latest update". Finally, we also include the entire body of the html page, which is the content of the article. That's where the most processing happens, because we have to replace the local image links with a full http reference, so that the content can stand alone from the location we gathered it from. Due to the transformations by the XML library functions, we also have to clean various special characters.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># function to read metadata from posts</span>
metadata &lt;-<span class="st"> </span>function(<span class="dt">path=</span><span class="st">"./"</span>,<span class="dt">base=</span><span class="st">""</span>){
    <span class="co"># create a structure to hold info</span>
    meta &lt;-<span class="st"> </span><span class="kw">list</span>(<span class="dt">post=</span><span class="st">""</span>, <span class="dt">relpath=</span><span class="st">""</span>, <span class="dt">mdate=</span><span class="dv">0</span>, <span class="dt">title=</span><span class="st">""</span>, 
                 <span class="dt">author=</span><span class="st">""</span>,<span class="dt">date=</span><span class="st">""</span>, <span class="dt">description=</span><span class="st">""</span>, <span class="dt">keywords=</span><span class="st">""</span>,
                 <span class="dt">content=</span><span class="st">""</span>)
                
    <span class="co"># get list of post html files</span>
    dirs &lt;-<span class="st"> </span><span class="kw">list.dirs</span>(path,<span class="dt">recursive=</span><span class="ot">FALSE</span>,<span class="dt">full.names=</span><span class="ot">FALSE</span>)
    postpaths &lt;-<span class="st"> </span><span class="kw">paste</span>(dirs,<span class="st">"/"</span>,dirs,<span class="st">".html"</span>,<span class="dt">sep=</span><span class="st">''</span>)
    postfullpaths &lt;-<span class="st"> </span><span class="kw">paste</span>(path,postpaths,<span class="dt">sep=</span><span class="st">''</span>)
    
    <span class="co"># extract meta data from each file</span>
    for(i in <span class="dv">1</span>:<span class="kw">length</span>(postfullpaths)){
        if(<span class="kw">file.exists</span>(postfullpaths[i])){
            meta$post[i] &lt;-<span class="st"> </span>dirs[i]
            meta$relpath[i] &lt;-<span class="st"> </span>postpaths[i]
            meta$mdate[i] &lt;-<span class="st"> </span><span class="kw">file.info</span>(postfullpaths[i])$mtime 
            doc &lt;-<span class="st"> </span><span class="kw">htmlParse</span>(postfullpaths[i])
            <span class="kw">absolurl</span>(doc,<span class="dt">base=</span><span class="kw">paste</span>(base,dirs[i],<span class="st">"/"</span>,<span class="dt">sep=</span><span class="st">""</span>))
            xvalue &lt;-<span class="st"> </span><span class="kw">querySelector</span>(doc,<span class="st">"title"</span>)
            if(!<span class="kw">is.null</span>(xvalue)) meta$title[i] &lt;-<span class="st"> </span><span class="kw">xmlValue</span>(xvalue)
            meta$author[i] &lt;-<span class="st"> </span><span class="kw">metaval</span>(doc,<span class="st">"author"</span>)
            meta$date[i] &lt;-<span class="st"> </span><span class="kw">metaval</span>(doc,<span class="st">"date"</span>)
            meta$description[i] &lt;-<span class="st"> </span><span class="kw">metaval</span>(doc,<span class="st">"description"</span>)
            meta$keywords[i] &lt;-<span class="st"> </span><span class="kw">metaval</span>(doc,<span class="st">"keywords"</span>)
            
            <span class="co"># extract a clean-ish copy of the content</span>
            body &lt;-<span class="st"> </span><span class="kw">toString.XMLNode</span>(<span class="kw">querySelector</span>(doc,<span class="st">"body"</span>))
            body &lt;-<span class="st"> </span><span class="kw">substr</span>(body,<span class="dv">7</span>,<span class="kw">nchar</span>(body)-<span class="dv">8</span>)
            body &lt;-<span class="st"> </span><span class="kw">gsub</span>(<span class="st">"&amp;#13;"</span>,<span class="st">""</span>,body)
            body &lt;-<span class="st"> </span><span class="kw">gsub</span>(<span class="st">"</span><span class="ch">\x92</span><span class="st">"</span>,<span class="st">"'"</span>,body)
            body &lt;-<span class="st"> </span><span class="kw">gsub</span>(<span class="st">"[</span><span class="ch">\x93</span><span class="st">-</span><span class="ch">\x94</span><span class="st">]"</span>,<span class="st">'"'</span>,body)
            body &lt;-<span class="st"> </span><span class="kw">gsub</span>(<span class="st">"</span><span class="ch">\x96</span><span class="st">"</span>,<span class="st">'&amp;mdash;'</span>,body)
            body &lt;-<span class="st"> </span><span class="kw">gsub</span>(<span class="st">"</span><span class="ch">\x85</span><span class="st">"</span>,<span class="st">'&amp;#8230;'</span>,body)
            body &lt;-<span class="st"> </span><span class="kw">gsub</span>(<span class="st">"[</span><span class="ch">\x7f</span><span class="st">-</span><span class="ch">\xff</span><span class="st">]"</span>,<span class="st">' '</span>,body)
            g &lt;-<span class="st"> </span><span class="kw">regexpr</span>(<span class="st">"&lt;!--DISQUS"</span>,body)[<span class="dv">1</span>] -<span class="st"> </span><span class="dv">1</span>
            if(g&gt;<span class="dv">0</span>) body &lt;-<span class="st"> </span><span class="kw">substr</span>(body,<span class="dv">1</span>,g)
            meta$content[i] &lt;-<span class="st"> </span>body
        }
    }
    <span class="kw">return</span>(meta)    
}</code></pre>
<p>The metadata function uses a special helper function for obtaining the value of a metadata field. That's shown below.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># get a single metadata value</span>
metaval &lt;-<span class="st"> </span>function(xdoc,meta){
            r &lt;-<span class="st"> ""</span>
            xvalue &lt;-<span class="st"> </span><span class="kw">querySelector</span>(xdoc,<span class="kw">paste</span>(<span class="st">"meta[name="</span>,meta,<span class="st">"]"</span>,<span class="dt">sep=</span><span class="st">""</span>))
            if(!<span class="kw">is.null</span>(xvalue)) r &lt;-<span class="st"> </span><span class="kw">xmlGetAttr</span>(xvalue,<span class="st">"content"</span>,<span class="dt">default=</span><span class="st">""</span>)
            <span class="kw">return</span>(r)
}</code></pre>
<p>The metadata function also grabs the content of the html page, which anticipates that we might want to generate an rss feed. However, the contents may contain relative links, for example to images. These links neeed to be made into absolute urls (i.e with full http path). The function below takes care of the main links. Essentially, it prefixes the base URL to the relative link in the attribute of the html tag.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># convert links in an xml node like &lt;body&gt; to </span>
<span class="co"># absolute references, using a base url</span>
absolurl &lt;-<span class="st"> </span>function(xml,<span class="dt">base=</span><span class="st">""</span>){
    if(<span class="kw">substr</span>(base,<span class="kw">nchar</span>(base),<span class="kw">nchar</span>(base)) !=<span class="st"> "/"</span>) 
        base &lt;-<span class="st"> </span><span class="kw">paste</span>(base,<span class="st">"/"</span>,<span class="dt">sep=</span><span class="st">""</span>)

    <span class="co"># expand and link (stylesheet) references</span>
    links &lt;-<span class="st"> </span><span class="kw">querySelectorAll</span>(xml,<span class="st">"link"</span>)
    for(i in links){
        link &lt;-<span class="st"> </span><span class="kw">xmlGetAttr</span>(i,<span class="st">"href"</span>,<span class="dt">default=</span><span class="st">""</span>)
        if(link !=<span class="st"> ""</span> &amp;<span class="st"> </span><span class="kw">substr</span>(link,<span class="dv">1</span>,<span class="dv">4</span>) !=<span class="st"> "http"</span>){
            <span class="kw">removeAttributes</span>(i,<span class="dt">.attrs=</span><span class="st">"href"</span>)
            <span class="kw">addAttributes</span>(i,<span class="dt">href=</span><span class="kw">paste</span>(base,link,<span class="dt">sep=</span><span class="st">""</span>))
        }
    }

    <span class="co"># expand img links to absolute references</span>
    imgs &lt;-<span class="st"> </span><span class="kw">querySelectorAll</span>(xml,<span class="st">"img"</span>)
    for(i in imgs){
        link &lt;-<span class="st"> </span><span class="kw">xmlGetAttr</span>(i,<span class="st">"src"</span>,<span class="dt">default=</span><span class="st">""</span>)
        if(link !=<span class="st"> ""</span> &amp;<span class="st"> </span><span class="kw">substr</span>(link,<span class="dv">1</span>,<span class="dv">4</span>) !=<span class="st"> "http"</span>){
            <span class="kw">removeAttributes</span>(i,<span class="dt">.attrs=</span><span class="st">"src"</span>)
            <span class="kw">addAttributes</span>(i,<span class="dt">src=</span><span class="kw">paste</span>(base,link,<span class="dt">sep=</span><span class="st">""</span>))
        }
    }
    
    <span class="co"># expand a a-links that do not have https or # prefix</span>
    anchors &lt;-<span class="st"> </span><span class="kw">querySelectorAll</span>(xml,<span class="st">"a"</span>)
    for(i in anchors){
        link &lt;-<span class="st"> </span><span class="kw">xmlGetAttr</span>(i,<span class="st">"href"</span>,<span class="dt">default=</span><span class="st">""</span>)
        if(link !=<span class="st"> ""</span> &amp;<span class="st"> </span><span class="kw">substr</span>(link,<span class="dv">1</span>,<span class="dv">4</span>) !=<span class="st"> "http"</span> 
           &amp;<span class="st"> </span><span class="kw">substr</span>(link,<span class="dv">1</span>,<span class="dv">1</span>) !=<span class="st">"#"</span>){
            <span class="kw">removeAttributes</span>(i,<span class="dt">.attrs=</span><span class="st">"href"</span>)
            <span class="kw">addAttributes</span>(i,<span class="dt">src=</span><span class="kw">paste</span>(base,link,<span class="dt">sep=</span><span class="st">""</span>))
        }
    }
}</code></pre>
<p>Once we have all the meta-data, it's relatively simple to generate an rss xml file. To take care of the xml pre-amble, we'll just read a template file (or an existing rss file if it exists). Then, for each item in the meta data list, we'll generate an xml node called <code>&lt;item&gt;</code>, which is how rss stores the article data. Note that if the metadata isn't passed as a parameter, then we construct it within the <code>rssupdate</code> function, using the <code>path</code> and <code>base</code> parameters. This gives some flexibility to use the <code>rssupdate</code> function standalone, or as part of a larger process where we don't want to duplicate reading the metadata.</p>
<p>Finally, the <code>rssupdate</code> function also allows for a rudimentary filtering on a keyword. This is useful if you want to create an rss feed for articles that contain a specific keyword. For example, if you put "R-code" as a keyword in your articles that contain R, you can then create a feed for <a href="http://www.r-bloggers.com/">R-bloggers</a>.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># create rss feed</span>
rssupdate &lt;-<span class="st"> </span>function(filename,<span class="dt">meta=</span><span class="ot">NA</span>,<span class="dt">path=</span><span class="ot">NA</span>,<span class="dt">base=</span><span class="st">""</span>,<span class="dt">keyword=</span><span class="ot">NA</span>){
    <span class="co"># get sorted post metadata</span>
    if(<span class="kw">typeof</span>(meta)!=<span class="st">"list"</span>) 
        meta &lt;-<span class="st"> </span><span class="kw">metadata</span>(<span class="kw">paste</span>(path,<span class="st">"posts/"</span>,<span class="dt">sep=</span><span class="st">""</span>),<span class="dt">base=</span>base)
    meta &lt;-<span class="st"> </span><span class="kw">as.data.frame</span>(meta)
    if(!<span class="kw">is.na</span>(keyword)) p &lt;-<span class="st"> </span><span class="kw">which</span>(<span class="kw">grepl</span>(keyword,meta$keywords))
    else p &lt;-<span class="st"> </span><span class="dv">1</span>:<span class="kw">nrow</span>(meta)
    
    <span class="co"># rss preamble</span>
    doc &lt;-<span class="st"> </span><span class="kw">xmlParse</span>(<span class="kw">paste</span>(path,filename,<span class="dt">sep=</span><span class="st">""</span>))
    url &lt;-<span class="st"> </span><span class="kw">xmlValue</span>(<span class="kw">querySelector</span>(doc,<span class="st">"link"</span>))
    if(<span class="kw">substr</span>(url,<span class="kw">nchar</span>(url),<span class="kw">nchar</span>(url)) !=<span class="st"> "/"</span>) url &lt;-<span class="st"> </span><span class="kw">paste</span>(url,<span class="st">"/"</span>,<span class="dt">sep=</span><span class="st">""</span>)
    
    channel &lt;-<span class="st"> </span><span class="kw">xmlParent</span>(<span class="kw">querySelector</span>(doc,<span class="st">"item"</span>))
    <span class="kw">removeNodes</span>(<span class="kw">querySelectorAll</span>(doc,<span class="st">"item"</span>))
    
    <span class="co"># create new item nodes</span>
    for(i in p){
        a &lt;-<span class="st"> </span><span class="kw">newXMLNode</span>(<span class="st">"item"</span>)
        title &lt;-<span class="st"> </span><span class="kw">newXMLNode</span>(<span class="st">"title"</span>, <span class="kw">newXMLTextNode</span>(meta$title[i]))
        link &lt;-<span class="st"> </span><span class="kw">newXMLNode</span>(<span class="st">"link"</span>, <span class="kw">newXMLTextNode</span>(
            <span class="kw">paste</span>(url,<span class="st">"posts/"</span>,meta$relpath[i],<span class="dt">sep=</span><span class="st">""</span>)
        ))
        guid &lt;-<span class="st"> </span><span class="kw">newXMLNode</span>(<span class="st">"guid"</span>, <span class="kw">newXMLTextNode</span>(
            <span class="kw">paste</span>(url,<span class="st">"posts/"</span>,meta$relpath[i],<span class="dt">sep=</span><span class="st">""</span>)
        ))
        pubDate &lt;-<span class="st"> </span><span class="kw">newXMLNode</span>(<span class="st">"pubDate"</span>, <span class="kw">newXMLTextNode</span>(
            <span class="kw">format</span>(<span class="kw">as.POSIXct</span>(meta$date[i]),<span class="st">"%a, %d %b %Y %H:%M:%S %z"</span>)
        ))
        description &lt;-<span class="st"> </span><span class="kw">newXMLNode</span>(<span class="st">"description"</span>, <span class="kw">newXMLTextNode</span>(
            meta$description[i]
        ))
        content &lt;-<span class="st"> </span><span class="kw">newXMLNode</span>(<span class="st">"content:encoded"</span>, 
            <span class="kw">newXMLTextNode</span>(meta$content[i], <span class="dt">cdata=</span><span class="ot">TRUE</span>
        ))
        <span class="kw">addChildren</span>(channel,a)
        <span class="kw">addChildren</span>(a,<span class="dt">kids =</span> 
            <span class="kw">list</span>(title,link,guid,pubDate,description,content)
        )
    }
    <span class="kw">return</span>(doc)
}</code></pre>
<p>Finally, we use all the functions above in a "blog update" function. The function rewrites the <code>index.html</code> page and creates rss files.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># rewrite the index.html page of the blog</span>
<span class="co"># incl. making a backup copy </span>
blogupdate &lt;-<span class="st"> </span>function(<span class="dt">path=</span><span class="st">"./"</span>,<span class="dt">base=</span><span class="st">""</span>){
    curdir &lt;-<span class="st"> </span><span class="kw">getwd</span>()
    <span class="kw">setwd</span>(path)
    <span class="kw">file.copy</span>(<span class="dt">from=</span><span class="st">"index.html"</span>,<span class="dt">to=</span><span class="st">"index.html.old"</span>,<span class="dt">overwrite=</span><span class="ot">TRUE</span>)
    meta &lt;-<span class="st"> </span><span class="kw">metadata</span>(<span class="dt">path=</span><span class="st">"posts/"</span>,<span class="dt">base=</span><span class="kw">paste</span>(base,<span class="st">"posts/"</span>,<span class="dt">sep=</span><span class="st">""</span>))
    doc &lt;-<span class="st"> </span><span class="kw">reindex</span>(path,<span class="dt">meta=</span>meta)
    <span class="kw">saveXML</span>(doc,<span class="st">"index.html"</span>)
    <span class="kw">saveXML</span>(<span class="kw">rssupdate</span>(<span class="st">"template.xml"</span>,<span class="dt">path=</span>path,<span class="dt">meta=</span>meta),<span class="st">"all.xml"</span>)
    <span class="kw">saveXML</span>(<span class="kw">rssupdate</span>(<span class="st">"template.xml"</span>,<span class="dt">path=</span>path,<span class="dt">keyword=</span><span class="st">"R-code"</span>,<span class="dt">meta=</span>meta),<span class="st">"rcode.xml"</span>)
    <span class="kw">setwd</span>(curdir)   
}</code></pre>
<p>That's it &mdash; just create literate R articles in folders underneath the <code>posts</code> folder and run the <code>blogupdate</code> function. You then need to sync your files with the host site and that depends on where you're hosting your blog. If it is on GitHub then just use the relevant <code>git</code> command like "push" (or on Windows, the <code>sync</code> button).</p>
<div class="references">

</div>
<div class="footnotes">
<hr /><ol><li id="fn1"><p>I'm using Windows terminology of "folders" here, of course meaning "directories" for nix fans.<a href="#fnref1"><U+21A9></a></p></li>
<li id="fn2"><p>where <em>username</em> is your GitHub username.<a href="#fnref2"><U+21A9></a></p></li>
</ol></div>
<!-- GOOGLE ANALYTICS -->
<script><![CDATA[
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-60473518-1', 'auto');
  ga('send', 'pageview');

]]]]><![CDATA[></script>]]></content:encoded></item><item><title>Sushi Train</title><link>http://richardderozario.org/posts/SushiTrain/SushiTrain.html</link><guid>http://richardderozario.org/posts/SushiTrain/SushiTrain.html</guid><pubDate>Mon, 02 Mar 2015 00:00:00 +1100</pubDate><description></description><content:encoded><![CDATA[
<div id="header">
<h1 class="title">Sushi Train</h1>
<h3 class="date">2015-03-02</h3>
</div>
<p>We ate at the <a href="https://www.google.com.au/maps/place/Sakura+Kaiten+Sushi/@-37.812989,144.971679,15z/data=!4m2!3m1!1s0x0:0x90ab3fbe0265fb4b">Sakura Kaiten</a> sushi restaurant on Friday. It's almost a clich  that nerds like trains, so eating at a restaurant that delivers a continuous stream of delights through a rotary conveyor belt is a particular joy. I couldn't help myself, though. When V. remarked that she was waiting for her favourite crispy crab to come around, I started wondering about the analytics of the sushi train.</p>
<p>So, you've got people sitting around this rotating stream of little dishes, and everybody just takes the dish that they want the most at that moment. Or they wait to see if what they want will appear shortly. The chefs continuously replace dishes that have been taken &mdash; and add the occasional new and different dish. What are the chances that you'll wait more than one rotation and not see your favourite?</p>
<p>So, let's create a vector of seat positions and assign some dishes to them. We'll also maintain a list of which person (on the seat) is currently eating. At the start, nobody is eating yet. For simplicity, I will start with five seats and 10 dishes.</p>
<pre class="sourceCode r"><code class="sourceCode r">    <span class="co"># seats and dishes</span>
    nSeats &lt;-<span class="st"> </span><span class="dv">5</span>         <span class="co"># number of seats</span>
    nDishes &lt;-<span class="st"> </span><span class="dv">10</span>       <span class="co"># number of dishes</span>
    dishes &lt;-<span class="st"> </span><span class="dv">1</span>:nDishes <span class="co"># ready dishes</span>
    
    <span class="co"># start with preferred dishes in front of seats</span>
    xdish &lt;-<span class="st"> </span><span class="dv">1</span>:nSeats
    seats &lt;-<span class="st"> </span>dishes[xdish]           <span class="co"># give dishes to seat</span>
    dishes[xdish] &lt;-<span class="st"> </span>-dishes[xdish]  <span class="co"># dishes not ready anymore</span>
    
    <span class="co"># who is eating?</span>
    eating &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>,nSeats)
    eatTime &lt;-<span class="st"> </span>nSeats               <span class="co"># eating time is one cycle</span></code></pre>
<p>Now, every diner has preferences for the dishes. Let's for the moment say that everybody starts with the same general preferences, indicated by the dish number &mdash; one for most preferred, two for next preferred, and so on. However, once a dish has been sampled, its preference drops to last. So, we have a rotating preference list per seat.</p>
<pre class="sourceCode r"><code class="sourceCode r">    <span class="co"># dish preference matrix</span>
    prefs &lt;-<span class="st"> </span><span class="kw">t</span>(<span class="kw">matrix</span>(<span class="dv">1</span>:nDishes,<span class="dt">nrow=</span>nDishes,<span class="dt">ncol=</span>nSeats))
    
    <span class="co"># rotate a vector of e.g. preferences</span>
    rotvec &lt;-<span class="st"> </span>function(x){ <span class="kw">c</span>(x[<span class="dv">2</span>:<span class="kw">length</span>(x)],x[<span class="dv">1</span>])}</code></pre>
<p>Now, at each turn, if the dish in front of the seat matches the first preference for that seat, we take it and rotate our preference. We then shift the dishes along the seats. The chef sits at seat one. When a "taken" dish arrives, the chef will replace it, if a matching dish is ready. Otherwise, the chef will let the taken dish pass, but will prepare a matching dish for the next time the taken dish comes around. Making a dish takes time, so we'll keep track of that as well. For simplicity, we'll measure duration in terms of the number of dishes that pass &mdash; so a "cycle" is the length of the train (i.e. the number of seats).</p>
<p>If a seat has taken a dish, it goes into eating mode and will skip the next <span class="math">\(n\)</span> dishes that pass by. On the other hand, if the dish in front of the seat doesn't match our first preference, then we wait for the next dish (and increase our count of the numnber of dishes that passed before we got our choice). We reseat the count if we take a dish.</p>
<pre class="sourceCode r"><code class="sourceCode r">    <span class="co"># count of dishes passed for each seat</span>
    passed &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>,nSeats)     <span class="co"># for each seat how long have we waited?</span>
    genpassed &lt;-<span class="st"> </span><span class="kw">c</span>()            <span class="co"># in general, how long do we wait?</span>
    making &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>,nDishes)    <span class="co"># which dishes are we making?</span>
    makeTime &lt;-<span class="st"> </span>nSeats          <span class="co"># making a dish takes one cycle</span>
    
    
    <span class="co"># process one turn </span>
    turn &lt;-<span class="st"> </span>function(){
        <span class="co"># at chef's seat</span>
        if(seats[<span class="dv">1</span>] &lt;<span class="st"> </span><span class="dv">0</span>){ <span class="co"># there's eaten dish</span>
            x &lt;-<span class="st"> </span><span class="kw">which</span>(dishes ==<span class="st"> </span>-seats[<span class="dv">1</span>])
            if(<span class="kw">length</span>(x)&gt;<span class="dv">0</span>){ <span class="co"># there's a replacement ready</span>
                seats[<span class="dv">1</span>] &lt;&lt;-<span class="st"> </span>-seats[<span class="dv">1</span>]
                dishes[x[<span class="dv">1</span>]] &lt;&lt;-<span class="st"> </span>-dishes[x[<span class="dv">1</span>]]
                making[x[<span class="dv">1</span>]] &lt;&lt;-<span class="st"> </span>makeTime
            } else {    <span class="co"># there's no replacement ready</span>
                x &lt;-<span class="st"> </span><span class="kw">which</span>(dishes ==<span class="st"> </span>seats[<span class="dv">1</span>])
                if(<span class="kw">length</span>(x)&gt;<span class="dv">0</span>) {
                    if(making[x[<span class="dv">1</span>]] &gt;<span class="st"> </span><span class="dv">0</span>) making[x[<span class="dv">1</span>]] &lt;&lt;-<span class="st"> </span>making[x[<span class="dv">1</span>]] -<span class="dv">1</span>
                    else dishes[x[<span class="dv">1</span>]] &lt;&lt;-<span class="st"> </span>-dishes[x[<span class="dv">1</span>]]
                }
            }
        } else {    <span class="co"># no eaten dish, but prep replacements</span>
            x &lt;-<span class="st"> </span><span class="kw">which</span>(dishes &lt;<span class="st"> </span><span class="dv">0</span>)
            if(<span class="kw">length</span>(x)&gt;<span class="dv">0</span>){    <span class="co"># there is a dish to prep</span>
                if(making[x[<span class="dv">1</span>]] &gt;<span class="st"> </span><span class="dv">0</span>) making[x[<span class="dv">1</span>]] &lt;&lt;-<span class="st"> </span>making[x[<span class="dv">1</span>]] -<span class="dv">1</span>
                else dishes[x[<span class="dv">1</span>]] &lt;&lt;-<span class="st"> </span>-dishes[x[<span class="dv">1</span>]]
            } else {        <span class="co"># no dish to prep, but replace one</span>
                x &lt;-<span class="st"> </span>prefs[<span class="dv">1</span>,<span class="dv">1</span>]
                prefs[<span class="dv">1</span>,] &lt;&lt;-<span class="st"> </span><span class="kw">rotvec</span>(prefs[<span class="dv">1</span>,])
                seats[<span class="dv">1</span>] &lt;&lt;-<span class="st"> </span>dishes[x]
                dishes[x] &lt;&lt;-<span class="st"> </span>-dishes[x]
            }
        }
        
        <span class="co"># at diners' seats</span>
        for(i in <span class="dv">2</span>:nSeats){
            if(eating[i]==<span class="dv">0</span>){
                if(seats[i] ==<span class="st"> </span>prefs[i,<span class="dv">1</span>]){ <span class="co"># yeah! preference is here</span>
                    seats[i] &lt;&lt;-<span class="st"> </span>-seats[i]
                    prefs[i,] &lt;&lt;-<span class="st"> </span><span class="kw">rotvec</span>(prefs[i,])
                    genpassed &lt;&lt;-<span class="st"> </span><span class="kw">c</span>(genpassed,passed[i])
                    passed[i] &lt;&lt;-<span class="st"> </span><span class="dv">0</span>
                    eating[i] &lt;&lt;-<span class="st"> </span>eatTime
                } else {                    <span class="co"># pass: not preference</span>
                    passed[i] &lt;&lt;-<span class="st"> </span>passed[i] +<span class="st"> </span><span class="dv">1</span>
                }
            } else {    <span class="co"># we're eating: count it down</span>
                eating[i] &lt;&lt;-<span class="st"> </span>eating[i] -<span class="dv">1</span> 
            }
        }
        
        <span class="co"># rotate the dishes through the seats</span>
        seats &lt;&lt;-<span class="st"> </span><span class="kw">rotvec</span>(seats)
    }</code></pre>
<p>For the small number of diners and dishes, the cycle isn't very long. If we simulate a large number of turns, we would see that, on average, you would expect to wait a little bit less than one cycle to get your preferred dish. That changes quickly as the number of seats increases.</p>
<div class="figure">
<img src="http://richardderozario.org/posts/SushiTrain/figure/hist-1.png" /></div>
<div class="references">

</div>
<!-- GOOGLE ANALYTICS -->
<script><![CDATA[
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-60473518-1', 'auto');
  ga('send', 'pageview');

]]]]><![CDATA[></script>]]></content:encoded></item><item><title>Systemic events</title><link>http://richardderozario.org/posts/SystemicEvents/SystemicEvents.html</link><guid>http://richardderozario.org/posts/SystemicEvents/SystemicEvents.html</guid><pubDate>Mon, 09 Mar 2015 00:00:00 +1100</pubDate><description>One of the most significant things that the Basel II accord did for Operational Risk was to call attention to its heavy tailed nature. An organisation may have years of almost trivial losses, and then experience a loss event that is hundreds of times larger. This characteristic of rare and very large losses presents quite a challenge to quantitative models. Even fitting the data to heavy-tailed distributions doesn’t usually solve the problem, and most loss-distribution based models will try to model the tail events separately. However, there is not a lot of theory underneath the modelling that explains why the tail might be different. A possible explanation is hinted at in casual conversation about operational losses, namely “systemic events” – where multiple unit components (like multiple transactions, multiple customers, or multiple businesses) are affected by a single event. In this blog post, I show with some simulations that systemic events could “hide” in the tail of ordinary loss distributions. Most loss-distribution approaches do not try to capture the underlying mechanism that might explain why the tail is different. However, without representing such mechanisms, such as systemic loss events, the models are risking their validity.</description><content:encoded><![CDATA[
<div id="header">
<h1 class="title">Systemic events</h1>
<h3 class="date">2015-03-09</h3>
</div>
<p>One of the most significant things that the Basel II accord did for Operational Risk was to call attention to its heavy tailed nature. An organisation may have years of almost trivial losses, and then experience a loss event that is hundreds of times larger. This characteristic of rare and very large losses presents quite a challenge to quantitative models. Even fitting the data to heavy-tailed distributions doesn't usually solve the problem, and most loss-distribution based models will try to model the tail events separately. However, there is not a lot of theory underneath the modelling that explains why the tail might be different.</p>
<p>There is a lot of talk about mixture models, lack of data, correlations and so forth, but I wondered what a mechanism might be that would justify our supposition that the tail events are fundamentally different, yet intrinsically tied to the other losses. A possible explanation is hinted at in casual conversation about operational losses, namely "systemic events" &mdash; where multiple unit components (like multiple transactions, multiple customers, or multiple businesses) are affected by a single event.</p>
<p>For example, imagine a payments process. The stream of individual transactions will experience occasional losses, typically due to keying errors or similar mistakes. However, the process is also part of a network of other processes, such as customer on-boarding, setting fee and interest rates, and so forth. In hierarchical fashion, some processes can affect multiple lower level transactions.</p>
<p>So, what might loss distributions look like if you assume systemic effects, like a process hierarchy?</p>
<p>Let's simulate an example, with some (I hope) reasonable assumptions. To start, assume a business process that is hierarchically organised, like the payments example above. Next, let's say that loss events (errors, frauds, interruptions, etc.) occur in the business operations at a rate of, say 360 per year, with a systemic event about 4 times a year. If an event occurs at a higher level process, then it affects multiple lower-level transactions &mdash; in effect, it becomes multiple events.</p>
<p>We can model the systemic hierarchy using the <a href="http://richardderozario.org/posts/karyl/karyl.html">k-ary level probabilities</a> that I wrote about in an earlier post. Essentially, the k-ary hierarchy acts as an upper limit for the more random hierarchy that might exist in reality. The closest k-ary tree for our assumptions will have a group size of 100 and a maximum level of 3.</p>
<p>Experience shows that the impacts of operational losses are typically heavy-tailed (even without systemic events). I'll chose the common lognormal distribution with a meanlog of 7 (about $1000) and standard deviation (log) of 2. For the purposes of analysis, the parameter choices don't matter, except in relation to the systemic effects (more on that later).</p>
<p>For simplicity, we'll keep the annual rate of events a constant, although for realistic models that would also be a random variable &mdash; typically Poisson or Negative Binomially distributed. Keeping the rate of events constant allows us to focus on the impact. With the systemic modification, the impact formulation can be summarised as:</p>
<p><span class="math">\[
Impact = \sum^K_i X
\]</span> where <span class="math">\(K\)</span> is a random variable following the k-rary level distribution and <span class="math">\(X\)</span> is a random variable following the log-normal distribution.</p>
<p>We'll generate the results in a table, so that we can distinguish the systemic from single events.</p>
<pre class="sourceCode r"><code class="sourceCode r">    <span class="kw">source</span>(<span class="st">"../karyl/karyl.R"</span>)  <span class="co"># use k-ary tree probability</span>
    <span class="kw">set.seed</span>(<span class="dv">123</span>)   <span class="co"># make this single simulation reproducible</span>
    
    gsize &lt;-<span class="st"> </span><span class="dv">100</span>    <span class="co"># group size</span>
    Lmax &lt;-<span class="st"> </span><span class="dv">3</span>       <span class="co"># max level</span>
    nSim =<span class="st"> </span><span class="dv">30</span>*<span class="dv">12</span>    <span class="co"># one year of events</span>
    Err &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">Impact=</span>gsize^(Lmax-<span class="kw">rkaryl</span>(nSim,Lmax,gsize)), <span class="dt">Systemic=</span><span class="ot">FALSE</span>)
    Err$Systemic[Err$Impact&gt;<span class="dv">1</span>] &lt;-<span class="st"> </span><span class="ot">TRUE</span>
    Err$Impact &lt;-<span class="st"> </span><span class="kw">sapply</span>(Err$Impact,function(i){<span class="kw">sum</span>(<span class="kw">rlnorm</span>(i,<span class="dt">meanlog=</span><span class="dv">7</span>,<span class="dt">sdlog=</span><span class="dv">2</span>))})</code></pre>
<p>The distribution of the impact of the errors is easiest to see on a log-scale. In addition to the histogram, I'll draw some red vertical lines so that we can see where the systemic losses are.</p>
<pre class="sourceCode r"><code class="sourceCode r">    <span class="co"># distribution of loss event impact</span>
    <span class="kw">source</span>(<span class="st">"../loglabels/loglabels.R"</span>)
    br&lt;-<span class="kw">hist</span>(<span class="kw">log</span>(Err$Impact,<span class="dv">10</span>),<span class="dt">axes=</span>F,
        <span class="dt">main=</span><span class="st">"systemic loss effect"</span>, <span class="dt">xlab=</span><span class="st">"log of impact"</span>)$breaks
    <span class="kw">axis</span>(<span class="dv">1</span>,<span class="dt">at=</span>br,<span class="dt">labels=</span><span class="kw">loglabels</span>(br))
    <span class="kw">axis</span>(<span class="dv">2</span>)
    <span class="kw">abline</span>(<span class="dt">v=</span><span class="kw">log</span>(Err$Impact[Err$Systemic==<span class="ot">TRUE</span>],<span class="dv">10</span>),<span class="dt">col=</span><span class="st">"red"</span>)</code></pre>
<div class="figure">
<img src="http://richardderozario.org/posts/SystemicEvents/figure/hist-1.png" /></div>
<p>As the histogram shows, the systemic events are in the tail. You'd expect that, because impact of a systemic event is a multiple of the impact of single events. However, if the single events have a heavily skewed distribution, such as a log-normal, the expected loss of an individual transaction is usually a lot lower than the larger losses. This means that the systemic event can be expected to be a multiplication of small(er) losses, which probably means that the size of the systemic events overlaps with the tail of the individual events.</p>
<p>Now, with relatively few systemic events that affect many (but not usually close to all) transactions, would we detect them by just looking at the historical impact data? To get a better sense of this, let's simulate many years of just single loss events, as well as years of combined single and systemic events.</p>
<figure><pre class="sourceCode r"><code class="sourceCode r">    <span class="co"># qqplot of single and combined single &amp; systemic losses</span>
    x &lt;-<span class="st"> </span><span class="kw">replicate</span>(<span class="dv">10000</span>, <span class="kw">sum</span>(<span class="kw">rlnorm</span>(nSim, <span class="dt">meanlog=</span><span class="dv">7</span>, <span class="dt">sdlog=</span><span class="dv">2</span> )))
    
    y &lt;-<span class="st"> </span><span class="kw">replicate</span>(<span class="dv">10000</span>, <span class="kw">sum</span>(<span class="kw">rlnorm</span>(
        gsize^(Lmax-<span class="kw">rkaryl</span>(nSim,Lmax,gsize)),
        <span class="dt">meanlog=</span><span class="dv">7</span>, <span class="dt">sdlog=</span><span class="dv">2</span>)))
        
    <span class="kw">qqplot</span>(x,y,
        <span class="dt">main=</span><span class="st">"single vs. systemic &amp; single losses"</span>,
        <span class="dt">xlab=</span><span class="st">"single losses"</span>, 
        <span class="dt">ylab=</span><span class="st">"single &amp; systemic losses"</span>)
        
    <span class="kw">abline</span>(<span class="dv">0</span>,<span class="dv">1</span>)</code></pre>
<img src="http://richardderozario.org/posts/SystemicEvents/figure/comparison-1.png" /><figcaption><em>Figure 1. distribution comparison: qqplot of single vs. systemic &amp; single losses</em>
</figcaption></figure><p>With a straight comparison of many years single vs. combined systemic and single loss events, we should be able to detect the difference. As <em>Figure 1</em> shows, the distributions look the same for lower value losses, but the combined distribution has more higher value losses.</p>
<p>However, in practice there are far fewer years to compare and the annual rate of losses varies. Below is the comparison again, but now taking into account those more realistic aspects.</p>
<figure><pre class="sourceCode r"><code class="sourceCode r">    <span class="co"># qqplot of single and combined single and systemic losses</span>
    x &lt;-<span class="st"> </span><span class="kw">replicate</span>(<span class="dv">500</span>, <span class="kw">sum</span>(<span class="kw">rlnorm</span>(<span class="kw">rpois</span>(<span class="dv">1</span>,nSim), <span class="dt">meanlog=</span><span class="dv">7</span>, <span class="dt">sdlog=</span><span class="dv">2</span> )))
    
    y &lt;-<span class="st"> </span><span class="kw">replicate</span>(<span class="dv">500</span>, <span class="kw">sum</span>(<span class="kw">rlnorm</span>(
        gsize^(Lmax-<span class="kw">rkaryl</span>(<span class="kw">rpois</span>(<span class="dv">1</span>,nSim),Lmax,gsize)),
        <span class="dt">meanlog=</span><span class="dv">7</span>, <span class="dt">sdlog=</span><span class="dv">2</span>)))
        
    <span class="kw">qqplot</span>(x,y,
        <span class="dt">main=</span><span class="st">"single vs. systemic &amp; single losses"</span>,
        <span class="dt">xlab=</span><span class="st">"single losses"</span>, 
        <span class="dt">ylab=</span><span class="st">"single &amp; systemic losses"</span>)
        
    <span class="kw">abline</span>(<span class="dv">0</span>,<span class="dv">1</span>)</code></pre>
<img src="http://richardderozario.org/posts/SystemicEvents/figure/comparison2-1.png" /><figcaption><em>Figure 2. qqplot of single vs. systemic &amp; single losses: less data and random rate</em>
</figcaption></figure><p>In the case illustrated in <em>Figure 2</em>, the differences in the tail are less clear. We could well decide that the tail differences are just outliers and that the data fits a lognormal distribution of single events.</p>
<p>The problem of sparsity of tail events (by definition) is well known and most loss-distribution models in practice try to model the tail separately. However, most loss-distribution approaches do not, as far as I know, try to capture the underlying mechanism, such as systemic loss events, that might explain why the tail is different. Without representing such mechanisms, the models are risking their validity.</p>
<div class="references">

</div>

<!-- GOOGLE ANALYTICS -->
<script><![CDATA[
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-60473518-1', 'auto');
  ga('send', 'pageview');

]]]]><![CDATA[></script>]]></content:encoded></item><item><title>The Annual March</title><link>http://richardderozario.org/posts/TheAnnualMarch/TheAnnualMarch.html</link><guid>http://richardderozario.org/posts/TheAnnualMarch/TheAnnualMarch.html</guid><pubDate>Sat, 21 Jun 2014 00:00:00 +1000</pubDate><description>Happy birthday to Melbourne’s first quant, Georg von Neumayer</description><content:encoded><![CDATA[
<div id="header">
<h1 class="title">The Annual March</h1>
<h3 class="date">2014-06-21</h3>
</div>
<p>The next time you want to catch your breath from a busy morning of statistical modelling, head to <a href="https://www.flickr.com/search/?q=flagstaff%20gardens">Flagstaff Gardens</a>. You can buy a nice roll from Vic Markets and enjoy the fresh air and leafy surrounds. Go ahead, munch your lunch, look up at the fresh sky&#8230; and enjoy the hallowed grounds of Melbourne's first analytics colleague, <a href="http://en.wikipedia.org/wiki/Georg_von_Neumayer">Georg von Neumayer</a>.</p>
<p>We may call our work "predictive analytics" now, but that's just the latest spin. Trying to predict things with calculations based on careful data gathering goes back a long way. The forecasting of weather events qualifies as one of the ancient roots of our industry. Stemming from these roots is Georg's work in establishing Melbourne's first meteorological observatory at Flagstaff Gardens in 1858.</p>
<p>I came across Georg when I started wondering who the first quant was in Melbourne. But then I got curious about his work, because he predates the foundation of modern timeseries analysis by Yule in the late 1800s<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a>. It turns out that Georg used Bessel functions, which in his day would have been as innovative as random trees are to us. Here is a small section of his work<a href="#fn2" class="footnoteRef" id="fnref2"><sup>2</sup></a>:</p>
<pre><code>
    If S signifies any of the meteorological elements, 
    n the number of the month commencing with the 1st of January, 
    the annual march is expressed by the following formula of Bessel:

    S(n) = s* + u' sin{(n+1/2) 30  + v' - 15 } + u'' sin{(n+1/2) 60  + 
    v-30 } + u''' sin{(n+1/2) 90  + v''' - 45 } + ...

    By the aid of this formula, the monthly mean values for each 
    element are computed and compared with the actual mean values 
    observed, thereby affording a means for testing the reliability 
    of the formula.
</code>
</pre>
<p>Aside from the particular approach, the predictive work sounds very familiar. Georg also faced other familiar challenges. He had to get foreign investment to fund his work and people questioned whether his work had any practical use, and whether it was even valid<a href="#fn3" class="footnoteRef" id="fnref3"><sup>3</sup></a>. Georg would be right at home in the analytics community of today's Melbourne.</p>
<p>Happy Birthday, Georg.</p>
<div class="references">

</div>
<div class="footnotes">
<hr /><ol><li id="fn1"><p>Terence Mills (2011) <em><a href="http://www.amazon.com/Foundations-Analysis-Palgrave-Advanced-Econometrics/dp/0230290183">Foundations of Modern Time Series Analysis</a></em>,Ch.2<a href="#fnref1"><U+21A9></a></p></li>
<li id="fn2"><p>G. Neumayer (1860) <a href="http://search.slv.vic.gov.au/primo_library/libweb/action/dlDisplay.do?vid=MAIN&amp;reset_config=true&amp;docId=SLV_VOYAGER1211729">Results of the magnetical, nautical and meteorological observations made and collected at the Flagstaff Observatory, Melbourne, and at various stations in the Colony of Victoria, March, 1858 to February, 1859</a><a href="#fnref2"><U+21A9></a></p></li>
<li id="fn3"><p>M.L.A. (1858, Aug 11) "<a href="http://trove.nla.gov.au/ndp/del/article/7299088?searchTerm=neumayer&amp;searchLimits=exactPhrase|||anyWords|||notWords|||requestHandler|||dateFrom=1858-08-11|||dateTo=1858-08-11|||l-advtitle=13|||l-advcategory=Article|||sortby">Professor Neumayer's observations</a>.", <a href="http://en.wikipedia.org/wiki/The_Argus_%28Melbourne%29">The Argus</a>, Melbourne<a href="#fnref3"><U+21A9></a></p></li>
</ol></div>
<!-- GOOGLE ANALYTICS -->
<script><![CDATA[
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-60473518-1', 'auto');
  ga('send', 'pageview');

]]]]><![CDATA[></script>]]></content:encoded></item><item><title>The value of advice</title><link>http://richardderozario.org/posts/ValueOfAdvice/ValueOfAdvice.html</link><guid>http://richardderozario.org/posts/ValueOfAdvice/ValueOfAdvice.html</guid><pubDate>Thu, 22 Jan 2015 00:00:00 +1100</pubDate><description>overview of state of research on decision support over the last decade</description><content:encoded><![CDATA[
<div id="header">
<h1 class="title">The value of advice</h1>
<h3 class="date">2015-01-22</h3>
</div>
<p>On one of my first consulting jobs I interviewed a scientist with the one of the government water authorities. At the end I said, "&#8230;so basically, you go around to the weirs, take samples, analyse the water quality to advise the minister, who then acts on that advice?"</p>
<p>"Yes," she said, "everything except the last bit."</p>
<p>Currently, I'm taking stock to see where the research on decision support has gotten to in the last decade or so &mdash; especially with regards to the core problem of "usage".</p>
<p>Most results were largely known at the turn of the century:</p>
<ol style="list-style-type: decimal"><li>Individuals tend to use bounded rationality and recognition-based decision strategies</li>
<li>Decision making processes in organisations are non-linear and often unclear in how (or if) information is actually used</li>
<li>Individuals tend to seek confirming information</li>
<li>Uptake of decision support systems is generally poor</li>
<li>and yet, and yet&#8230; models, even simple ones, tend to outperform intuition</li>
</ol><p>One of the most interesting reads is McCown's review of decision making by farmers in the USA and Australia<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a>. Amidst a broad review of the history of decision support systems for farmers, he observes that <em>normative</em> (that advise people on how they <em>should</em> decide) systems are less popular. However, this factor is mitigated by how much autonomy the decision maker has already. So, managers with less (perceived) autonomy are more likely to accept advise given by systems that are already part of the accepted norms of their organisations.</p>
<p>The issues of usage are compounded in an organisational setting. As the seminal article by Feldman and March<a href="#fn2" class="footnoteRef" id="fnref2"><sup>2</sup></a> pointed out, there is a surface structure in organisations for making decisions (procedures, committees, etc.) that doesn't seem align with the "reality" of how decisions are made.</p>
<p>Persistent problems like these have led to suggestions of different paradigms for decision making. For instance, McCown points to a more indirect approach, where the systems provide an opportunity for the decision maker to develop new mental models ("sense making") about the particular situation, options and uncertainties. From another perspective, Tsoukas sees no particular decisions being made in the classical sense of weighing options and making discrete choices, but rather the unfolding logic of a <em>practice</em>. Here, a practice is the following of a routine or ongoing process, solving breakdowns that occur in the process through practical coping.</p>
<p>Another perspective comes from the focus on Judge-Advocate System (JAS) of decision making. This models any situation where the decision maker (judge) takes advise, to some extent, from advisers (advocates). Bonaccio and Dalal<a href="#fn3" class="footnoteRef" id="fnref3"><sup>3</sup></a> provide an overview of the research up to 2005. Some of the findings, such as the tendency of judges to overweigh their own opinions to that of the advisers, and the improved accuracy of "averaging" different advice, complement findings of confirmation bias by other research. The discounting of advice in favour of one's own opinion is mediated by factors such as the balance of expertise between judge and advocate, the power of the judge, etc. A prominent factor is that advice that comes at some cost is discounted less.</p>
<p>These different perspectives lead to interesting approaches for decision support processes and infrastructures. For example, the sense making view, would support simulated environments for exploration, rather than pre-programmed solutions. The practice context would support embedding of decision support into the environment and infrastructure that is already part of organisational routines. Lastly, a fun thought is that the cost factor in advice discounting may mean that, all else being equal, it's better to provide advice as a fee charging consultant than as a freely accessible employee.</p>
<div class="references">

</div>
<div class="footnotes">
<hr /><ol><li id="fn1"><p>McCown R. L. (2005) "New Thinking About Farmer Decision Makers", in <em>The Farmer's Decision: Balancing Economic Successful Agriculture Production with Environmental Quality</em>, J.L. Hatfield (ed.), Soil and Water Conservation Society, Ankey, Iowa, USA, p11-44<a href="#fnref1"><U+21A9></a></p></li>
<li id="fn2"><p>Feldman M. S. and March J. G. (1981) "Information in Organizations as Signal and Symbol", Administrative Science Quarterly, V26N2, p171-186<a href="#fnref2"><U+21A9></a></p></li>
<li id="fn3"><p>Bonaccio, S. and Dalal R. (2006) "Advice taking and decision-making: An integrative literature review, and implications for the organizational sciences", <em>Organizational Behavior and Human Decision Processes 101</em>, p127-151<a href="#fnref3"><U+21A9></a></p></li>
</ol></div>
<!-- GOOGLE ANALYTICS -->
<script><![CDATA[
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-60473518-1', 'auto');
  ga('send', 'pageview');

]]]]><![CDATA[></script>]]></content:encoded></item></channel></rss>
