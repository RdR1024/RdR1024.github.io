{
    "contents" : "---\ntitle: \"Second order loss\"\nauthor: \"RdR\"\ndate: \"Sunday, January 25, 2015\"\noutput: html_document\n---\n\n```{r setup, echo=FALSE}\n# require(mc2d, quietly=TRUE)\nrequire(triangle, quietly=TRUE)\n```\n\nImagine a transactional process like credit card payments.  It will click along for many periods with relatively minor losses.  Then, as [Target experienced at Thanksgiving in 2013](http://en.wikipedia.org/wiki/History_of_Target_Corporation#2013_Security_Breach), there will be a massive event, with losses that are orders of magnitude larger. The example don't necessarily have to be a fraud.  For instance, we may have a transactional process where an error, like a legal documentation flaw, is not detected. After a while, the impact is suddenly felt accross many transactions.\n\nAn interesting feature of events like these is the systemic aspect.  The process has individual losses of a certain size and frequency, but the systemic events where there are multiples of those individual losses. It may also be the case that the range of individual impacts overlaps with the range of systemic impacts. That is, the larger of the individual losses may exceed the smaller of the systemic losses. \n\nNow the question is, how can we best model this kind of loss data?  To what extent is simple curve fitting of loss data still adequate, or do we need more sophisticated models to estimate losses?\n\nLet's model a simplified process: There will be a certain number of errors in each processing period. One in 1000 is a systemic error.  A systemic error means that between 10 and 1000, but most likely 100 transactions are affected. For both types of errors, the impact on an individual affected transaction is between 10 and 1M, but most likely 1000 dollars.\n\n```{r assumptions}\n    # loss process assumptions\n    syserrs <- 1 / 1000\n    sysqty  <- list(low=5,mode=10,high=100)\n    erramt  <- list(low=10, mode=1000, high=1e6)   # dollar impact\n```\n\nThe impact of the errors is split between the ordinary impact, for which we'll use a triangular distribution, and the systemic events. For the systemic impact we'll use a double triangular distribution, first to determine how many transactions are affected, and then to determine the impact for each transaction. The following are functions single and systemic losses.\n\n```{r lossfuncs}\n    # individual error impact function\n    singleimpact <- function(erramt,n=1){\n        return(rtriangle(n, erramt$low, erramt$high, erramt$mode))\n    }\n\n    # systemic impact function\n    sysimpact <- function(sysqty,erramt){\n        n <- rtriangle(1, sysqty$low,  sysqty$high, sysqty$mode)\n        return( sum( singleimpact(erramt,n)) )\n    }\n```\n\nWith these loss functions, we can now easily simulate loss events. We'll generate the results in a table, so that we can distinguish the single from systemic events.\n\n```{r sim}\n    Err <- data.table(Impact=replicate(1e4,singleimpact(erramt)), Systemic=F)\n\n    # random sample of one in 1000 (= 10) systemic errors\n    s <- sample(1:1e4, 10)\n    Err$Impact[s] <- sysimpact(sysqty, erramt)\n\n```\n\nWith these functions in place, it's simple to simulate a large number of errors and see what the effect of an occassional systemic event is.  The distribution of the impact of the errors is easiest to see on a log-scale.\n\n```{r sim}\n    # simulate 10,000 errors\n    z <- replicate(1e4, impact(erramt, syserrs, sysqty))\n    hist(log(z),breaks=100)\n```\n",
    "created" : 1422143873752.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "2407407809",
    "id" : "BC87AD22",
    "lastKnownWriteTime" : 1422255881,
    "path" : "X:/Google Drive/RdR/Proj/2ndorderloss/2ndorderloss.Rmd",
    "project_path" : "2ndorderloss.Rmd",
    "properties" : {
        "tempName" : "Untitled1"
    },
    "source_on_save" : false,
    "type" : "r_markdown"
}